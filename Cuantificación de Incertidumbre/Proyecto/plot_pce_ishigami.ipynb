{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Ishigami function (3 random inputs, scalar output)\n\nIn this example, we approximate the well-known Ishigami function with a total-degree Polynomial Chaos Expansion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import necessary libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport math\nimport numpy as np\nfrom UQpy.distributions import Uniform, JointIndependent\nfrom UQpy.surrogates import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define the Ishigami function, which reads:\n$f(x_1, x_2, x_3) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1)$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# function to be approximated\ndef ishigami(xx):\n    \"\"\"Ishigami function\"\"\"\n    a = 7\n    b = 0.1\n    term1 = np.sin(xx[0])\n    term2 = a * np.sin(xx[1])**2\n    term3 = b * xx[2]**4 * np.sin(xx[0])\n    return term1 + term2 + term3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Ishigami function has three random inputs, which are uniformly distributed in $[-\\pi, \\pi]$. Moreover, the\ninput random variables are mutually independent, which simplifies the construction of the joint distribution. Let's\ndefine the corresponding distributions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# input distributions\ndist1 = Uniform(loc=-np.pi, scale=2*np.pi)\ndist2 = Uniform(loc=-np.pi, scale=2*np.pi)\ndist3 = Uniform(loc=-np.pi, scale=2*np.pi)\nmarg = [dist1, dist2, dist3]\njoint = JointIndependent(marginals=marg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define our PCE. Only thing we need is the joint distribution.\n\nWe must now select a polynomial basis. Here we opt for a total-degree (TD) basis, such that the univariate\npolynomials have a maximum degree equal to $P$ and all multivariate polynomial have a total-degree\n(sum of degrees of corresponding univariate polynomials) at most equal to $P$. The size of the basis is then\ngiven by $\\frac{(N+P)!}{N! P!}$\nwhere $N$ is the number of random inputs (here, $N+3$).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# maximum polynomial degree\nP = 6\n\n# construct total-degree polynomial basis\npolynomial_basis = TotalDegreeBasis(joint, P)\n\n# check the size of the basis\nprint('Size of PCE basis:', polynomial_basis.polynomials_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We must now compute the PCE coefficients. For that we first need a training sample of input random variable\nrealizations and the corresponding model outputs. These two data sets form what is also known as an\n''experimental design''. It is generally advisable that the experimental design has $2-10$ times more data points\nthan the number of PCE polynomials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create training data\nsample_size = int(polynomial_basis.polynomials_number*5)\nprint('Size of experimental design:', sample_size)\n\n# realizations of random inputs\nxx_train = joint.rvs(sample_size)\n# corresponding model outputs\nyy_train = np.array([ishigami(x) for x in xx_train])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now fit the PCE coefficients by solving a regression problem. There are multiple ways to do this, e.g. least\nsquares regression, ridge regression, LASSO regression, etc. Here we opt for the _np.linalg.lstsq_ method, which\nis based on the _dgelsd_ solver of LAPACK.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# fit model\nleast_squares = LeastSquareRegression()\npce = PolynomialChaosExpansion(polynomial_basis=polynomial_basis, regression_method=least_squares)\n\npce.fit(xx_train, yy_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By simply post-processing the PCE's terms, we are able to get estimates regarding the mean and standard deviation\nof the model output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_est = pce.get_moments()[0]\nvar_est = pce.get_moments()[1]\nprint('PCE mean estimate:', mean_est)\nprint('PCE variance estimate:', var_est)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to the mean and variance estimates, we can very simply estimate the Sobol sensitivity indices, which\nquantify the importance of the input random variables in terms of impact on the model output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from UQpy.sensitivity import *\npce_sensitivity = PceSensitivity(pce)\npce_sensitivity.run()\nsobol_first = pce_sensitivity.first_order_indices\nsobol_total = pce_sensitivity.total_order_indices\nprint('First-order Sobol indices:')\nprint(sobol_first)\nprint('Total-order Sobol indices:')\nprint(sobol_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PCE should become increasingly more accurate as the maximum polynomial degree $P$ increases. We will test\nthat by computing the mean absolute error (MAE) between the PCE's predictions and the true model outputs, given a\nvalidation sample of $10^5$ data points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# validation data sets\nnp.random.seed(999)  # fix random seed for reproducibility\nn_samples_val = 100000\nxx_val = joint.rvs(n_samples_val)\nyy_val = np.array([ishigami(x) for x in xx_val])\n\nmae = []  # to hold MAE for increasing polynomial degree\nfor degree in range(16):\n    # define PCE\n    polynomial_basis = TotalDegreeBasis(joint, degree)\n    least_squares = LeastSquareRegression()\n    pce_metamodel = PolynomialChaosExpansion(polynomial_basis=polynomial_basis, regression_method=least_squares)\n\n    # create training data\n    np.random.seed(1)  # fix random seed for reproducibility\n    sample_size = int(pce_metamodel.polynomials_number * 5)\n    xx_train = joint.rvs(sample_size)\n    yy_train = np.array([ishigami(x) for x in xx_train])\n\n    # fit PCE coefficients\n    pce_metamodel.fit(xx_train, yy_train)\n\n    # compute mean absolute validation error\n    yy_val_pce = pce_metamodel.predict(xx_val).flatten()\n    errors = np.abs(yy_val.flatten() - yy_val_pce)\n    mae.append(np.linalg.norm(errors, 1) / n_samples_val)\n\n    print('Polynomial degree:', degree)\n    print('Mean absolute error:', mae[-1])\n    print(' ')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}